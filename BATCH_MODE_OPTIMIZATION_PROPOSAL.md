# Batch Mode Optimization Proposal
## Processing 500 Brands Efficiently

**Date:** 2026-02-17
**Target:** Reduce processing time from ~5.5 hours to ~45-90 minutes
**Goal:** Handle 500 brands with maximum time efficiency

---

## üìä Current State Analysis

### **Performance Profile (Per Brand)**
```
Single Brand Processing Time: ~40-45 seconds
- API Key Validation: 0.5s
- Data Collection (6 scrapers): 15-20s
  - Tavily Search: 8-12s (30 results)
  - Wikipedia/Web: 3-5s
  - Iranian Sources: 4-8s (Rasmio, Codal, etc.)
- Relationship Mapping: 5-8s (LLM call)
- Categorization: 3-5s (LLM call)
- Product Catalog: 4-6s (LLM call)
- Insights Generation: 6-8s (LLM call)
- Output Formatting: 2-3s (file I/O)
- Google Sheets Update: 1-2s
```

### **Current Batch Performance (500 Brands)**
```
Sequential Processing:
- Total Time: 500 √ó 40s = 20,000s = 5.5 hours
- Cost: 500 √ó $0.05 = $25
- API Calls: ~2,000 (Tavily) + ~2,000 (LLM) = ~4,000 total
- Success Rate: ~95% (with registry skipping unchanged brands)
```

### **Current Bottlenecks**

| Issue | Impact | Priority |
|-------|--------|----------|
| **Sequential Processing** | 5.5 hours for 500 brands | üî¥ CRITICAL |
| **API Key Validation per Brand** | 250s wasted (500 √ó 0.5s) | üü° MEDIUM |
| **Google Sheets per Brand** | 500-1000s overhead | üü° MEDIUM |
| **No Rate Limit Management** | Risk of API throttling | üü† HIGH |
| **No Progress Visibility** | User doesn't know ETA | üü° MEDIUM |
| **No Cost Tracking** | Can't monitor budget in real-time | üü° MEDIUM |
| **Agent Reinitialization** | Repeated setup overhead | üü¢ LOW |

---

## üöÄ Proposed Optimizations

### **1. Parallel Brand Processing** üî¥ CRITICAL

**Current:** Brands processed sequentially (one at a time)
**Proposed:** Process 5-10 brands in parallel with worker pool

**Benefits:**
- **5-10√ó faster**: 500 brands in 45-90 minutes (vs 5.5 hours)
- Maximize API throughput
- Better resource utilization

**Implementation:**
```python
import asyncio
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import Manager, Queue

def run_batch_mode_parallel(args):
    """Run batch processing with parallel workers."""
    # Configuration
    MAX_WORKERS = 8  # Parallel brand processing
    CHUNK_SIZE = 50  # Process in chunks for better memory management

    # Initialize shared resources
    registry = BrandRegistry()
    sheets_client = GoogleSheetsClient(args.sheets_credentials)
    brands = sheets_client.get_brands_from_sheet(args.sheets_id, start_row=2)

    # Filter brands that need processing
    brands_to_process = []
    for brand_data in brands:
        needs_proc, reason = registry.needs_processing(
            brand_data["brand_name"],
            brand_data.get("website"),
            brand_data.get("parent_company"),
            force=args.force
        )
        if needs_proc:
            brands_to_process.append(brand_data)

    logger.info(f"Processing {len(brands_to_process)}/{len(brands)} brands")
    logger.info(f"Workers: {MAX_WORKERS} parallel")
    logger.info(f"Estimated time: {len(brands_to_process) * 40 / MAX_WORKERS / 60:.1f} minutes")

    # Process in chunks
    results = {"success": [], "failed": [], "skipped": []}

    for chunk_start in range(0, len(brands_to_process), CHUNK_SIZE):
        chunk = brands_to_process[chunk_start:chunk_start + CHUNK_SIZE]

        # Process chunk in parallel
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {
                executor.submit(
                    process_single_brand,
                    brand_data,
                    args.sheets_credentials,
                    args.sheets_id
                ): brand_data for brand_data in chunk
            }

            for future in as_completed(futures):
                brand_data = futures[future]
                try:
                    result = future.result()
                    if result["status"] == "success":
                        results["success"].append(result)
                    elif result["status"] == "failed":
                        results["failed"].append(result)
                except Exception as e:
                    logger.error(f"Brand {brand_data['brand_name']} failed: {e}")
                    results["failed"].append({
                        "brand": brand_data["brand_name"],
                        "error": str(e)
                    })

        # Save checkpoint after each chunk
        save_checkpoint(results, chunk_start + len(chunk))

    return results

def process_single_brand(brand_data, sheets_creds, sheets_id):
    """Process a single brand (worker function)."""
    try:
        final_state = run_workflow(
            brand_name=brand_data["brand_name"],
            brand_website=brand_data.get("website"),
            parent_company=brand_data.get("parent_company"),
            google_sheets_credentials=sheets_creds,
            google_sheets_id=sheets_id
        )

        return {
            "status": "success" if not final_state.get("errors") else "failed",
            "brand": brand_data["brand_name"],
            "outputs": final_state.get("outputs", {}),
            "errors": final_state.get("errors", [])
        }
    except Exception as e:
        return {
            "status": "failed",
            "brand": brand_data["brand_name"],
            "error": str(e)
        }
```

**Expected Impact:**
- ‚è±Ô∏è **Time:** 5.5 hours ‚Üí 45-90 minutes (6-7√ó faster)
- üí∞ **Cost:** Same ($25 for 500 brands)
- üìà **Throughput:** 1 brand/40s ‚Üí 8 brands/40s

---

### **2. Smart Rate Limiting & Throttling** üü† HIGH

**Problem:** Risk of hitting API rate limits (OpenRouter, Tavily, Google Sheets)

**Solution:** Implement adaptive rate limiting with token bucket algorithm

```python
import time
from threading import Lock, Semaphore

class RateLimiter:
    """Thread-safe rate limiter for API calls."""

    def __init__(self, calls_per_minute=60, burst_size=10):
        self.calls_per_minute = calls_per_minute
        self.interval = 60.0 / calls_per_minute  # Seconds between calls
        self.burst_size = burst_size
        self.semaphore = Semaphore(burst_size)
        self.last_call = 0
        self.lock = Lock()

    def __call__(self, func):
        """Decorator for rate-limited functions."""
        def wrapper(*args, **kwargs):
            with self.lock:
                now = time.time()
                time_since_last = now - self.last_call

                if time_since_last < self.interval:
                    time.sleep(self.interval - time_since_last)

                self.last_call = time.time()

            return func(*args, **kwargs)
        return wrapper

# Usage
openrouter_limiter = RateLimiter(calls_per_minute=60, burst_size=10)
tavily_limiter = RateLimiter(calls_per_minute=100, burst_size=20)
sheets_limiter = RateLimiter(calls_per_minute=100, burst_size=20)

@openrouter_limiter
def llm_generate(prompt):
    return llm_client.generate(prompt)

@tavily_limiter
def tavily_search(query):
    return tavily.search(query)
```

**Configuration:**
```python
# config/settings.py
RATE_LIMITS = {
    "openrouter": {
        "calls_per_minute": 60,
        "burst_size": 10
    },
    "tavily": {
        "calls_per_minute": 100,
        "burst_size": 20
    },
    "google_sheets": {
        "calls_per_minute": 100,
        "burst_size": 20
    }
}
```

---

### **3. Batch Google Sheets Updates** üü° MEDIUM

**Current:** One API call per brand (500 calls)
**Proposed:** Batch updates every 10-20 brands (25-50 calls)

```python
class BatchSheetsClient:
    """Google Sheets client with batched updates."""

    def __init__(self, credentials_path, batch_size=20):
        self.client = GoogleSheetsClient(credentials_path)
        self.batch_size = batch_size
        self.pending_updates = []
        self.lock = Lock()

    def queue_update(self, sheet_id, row, status, outputs=None):
        """Queue an update for batch processing."""
        with self.lock:
            self.pending_updates.append({
                "sheet_id": sheet_id,
                "row": row,
                "status": status,
                "outputs": outputs
            })

            # Flush if batch size reached
            if len(self.pending_updates) >= self.batch_size:
                self.flush()

    def flush(self):
        """Execute all pending updates in a single batch."""
        if not self.pending_updates:
            return

        # Group by sheet_id
        updates_by_sheet = {}
        for update in self.pending_updates:
            sheet_id = update["sheet_id"]
            if sheet_id not in updates_by_sheet:
                updates_by_sheet[sheet_id] = []
            updates_by_sheet[sheet_id].append(update)

        # Execute batch updates
        for sheet_id, updates in updates_by_sheet.items():
            self.client.batch_update_status(sheet_id, updates)

        self.pending_updates = []
        logger.info(f"Flushed {len(updates)} sheet updates")
```

**Expected Impact:**
- ‚è±Ô∏è **Time Saved:** 500-750s (500 calls ‚Üí 25 calls @ 1-1.5s each)
- üìâ **API Quota:** 95% reduction in Google Sheets API usage

---

### **4. One-Time API Validation** üü° MEDIUM

**Current:** Validate API key for each brand (500 validations)
**Proposed:** Validate once at batch start

```python
def run_batch_mode_parallel(args):
    # Validate API keys ONCE before starting
    logger.info("Validating API keys...")

    try:
        llm_client.validate_api_key_with_test_call()
        logger.info("‚úÖ OpenRouter API key valid")
    except APIKeyError as e:
        logger.error("‚ùå OpenRouter API key invalid")
        print(str(e))
        sys.exit(1)

    # Validate Tavily if enabled
    if settings.TAVILY_API_KEY:
        try:
            tavily_client.validate()
            logger.info("‚úÖ Tavily API key valid")
        except TavilyAPIKeyError as e:
            logger.warning("‚ö†Ô∏è Tavily API key invalid - will skip Tavily search")

    # Now process brands (no per-brand validation)
    # ... parallel processing
```

**Expected Impact:**
- ‚è±Ô∏è **Time Saved:** 250s (500 √ó 0.5s)
- üí° **Better UX:** Immediate failure if keys invalid (fail fast at batch level)

---

### **5. Progress Tracking & ETA** üü° MEDIUM

**Problem:** No visibility during 5.5-hour runs

**Solution:** Real-time progress bar with ETA

```python
from tqdm import tqdm
import time

def run_batch_mode_parallel(args):
    brands_to_process = [...] # filtered brands

    # Initialize progress tracking
    progress = tqdm(
        total=len(brands_to_process),
        desc="Processing brands",
        unit="brand",
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
    )

    start_time = time.time()
    completed = 0

    # Process with progress updates
    for chunk in chunks(brands_to_process, CHUNK_SIZE):
        # ... parallel processing

        for future in as_completed(futures):
            result = future.result()
            completed += 1

            # Update progress
            elapsed = time.time() - start_time
            avg_time = elapsed / completed
            remaining = (len(brands_to_process) - completed) * avg_time

            progress.set_postfix({
                "Success": len(results["success"]),
                "Failed": len(results["failed"]),
                "ETA": f"{remaining/60:.1f}m",
                "Cost": f"${completed * 0.05:.2f}"
            })
            progress.update(1)

    progress.close()
```

**Output Example:**
```
Processing brands: 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë| 225/500 [32:15<38:42, 8.45s/brand]
Success: 220 | Failed: 5 | ETA: 38.7m | Cost: $11.25
```

---

### **6. Cost Tracking & Budget Limits** üü° MEDIUM

**Problem:** No real-time cost monitoring

**Solution:** Track costs and stop if budget exceeded

```python
class CostTracker:
    """Real-time cost tracking for batch processing."""

    def __init__(self, budget_limit=None):
        self.budget_limit = budget_limit
        self.costs = {
            "tavily": 0.0,
            "openrouter": 0.0,
            "total": 0.0
        }
        self.lock = Lock()

    def record_tavily_call(self, results_count=30):
        """Record cost of Tavily API call."""
        cost = results_count * 0.001  # $0.001 per result
        with self.lock:
            self.costs["tavily"] += cost
            self.costs["total"] += cost
            self._check_budget()

    def record_llm_call(self, tokens_used):
        """Record cost of LLM API call."""
        # Gemini Flash 3: $0.075 per 1M input tokens
        cost = tokens_used * 0.075 / 1_000_000
        with self.lock:
            self.costs["openrouter"] += cost
            self.costs["total"] += cost
            self._check_budget()

    def _check_budget(self):
        """Check if budget exceeded."""
        if self.budget_limit and self.costs["total"] > self.budget_limit:
            raise BudgetExceededError(
                f"Budget limit ${self.budget_limit:.2f} exceeded. "
                f"Current cost: ${self.costs['total']:.2f}"
            )

    def get_summary(self):
        """Get cost summary."""
        return {
            "tavily": f"${self.costs['tavily']:.2f}",
            "openrouter": f"${self.costs['openrouter']:.2f}",
            "total": f"${self.costs['total']:.2f}",
            "remaining": f"${self.budget_limit - self.costs['total']:.2f}" if self.budget_limit else "Unlimited"
        }

# Usage
cost_tracker = CostTracker(budget_limit=50.0)  # $50 limit

# In agents
cost_tracker.record_tavily_call(results_count=30)
cost_tracker.record_llm_call(tokens_used=5000)

# Check at any time
print(cost_tracker.get_summary())
# Output: {'tavily': '$15.00', 'openrouter': '$10.25', 'total': '$25.25', 'remaining': '$24.75'}
```

**CLI Integration:**
```bash
# Set budget limit
python main.py --google-sheets --budget 50.0

# Output during processing:
# [COST] Current: $12.50 / $50.00 (25%) | Tavily: $7.50 | LLM: $5.00
```

---

### **7. Smart Caching** üü¢ LOW

**Problem:** Redundant API calls for common data (industry taxonomies, etc.)

**Solution:** Cache common data across brands

```python
from functools import lru_cache
import hashlib
import json

class SmartCache:
    """Cache for expensive operations."""

    def __init__(self, cache_dir="cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def cache_key(self, operation, params):
        """Generate cache key from operation and params."""
        data = f"{operation}:{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(data.encode()).hexdigest()

    def get(self, operation, params):
        """Get cached result."""
        key = self.cache_key(operation, params)
        cache_file = self.cache_dir / f"{key}.json"

        if cache_file.exists():
            # Check if cache is fresh (< 24 hours)
            age = time.time() - cache_file.stat().st_mtime
            if age < 86400:  # 24 hours
                with open(cache_file) as f:
                    return json.load(f)
        return None

    def set(self, operation, params, result):
        """Cache result."""
        key = self.cache_key(operation, params)
        cache_file = self.cache_dir / f"{key}.json"

        with open(cache_file, 'w') as f:
            json.dump(result, f)

# Usage
cache = SmartCache()

def get_industry_taxonomy():
    """Get industry taxonomy (cached)."""
    cached = cache.get("industry_taxonomy", {})
    if cached:
        return cached

    # Fetch from API
    result = llm_client.generate("Return Iranian industry taxonomy...")

    # Cache for 24 hours
    cache.set("industry_taxonomy", {}, result)
    return result
```

**Expected Impact:**
- ‚è±Ô∏è **Time Saved:** 50-100s (avoid redundant LLM calls)
- üí∞ **Cost Saved:** $2-5 per batch

---

### **8. Enhanced Checkpoint System** üü° MEDIUM

**Current:** Checkpoint only on API errors
**Proposed:** Auto-checkpoint every N brands

```python
def run_batch_mode_parallel(args):
    CHECKPOINT_INTERVAL = 50  # Save every 50 brands

    for chunk_idx, chunk in enumerate(chunks(brands_to_process, CHUNK_SIZE)):
        # ... process chunk

        # Auto-checkpoint
        if (chunk_idx + 1) * CHUNK_SIZE % CHECKPOINT_INTERVAL == 0:
            save_checkpoint(results, completed_count=(chunk_idx + 1) * CHUNK_SIZE)
            logger.info(f"‚úì Checkpoint saved ({completed_count} brands)")

def save_checkpoint(results, completed_count, reason="auto"):
    """Save processing checkpoint."""
    checkpoint = {
        "timestamp": datetime.now().isoformat(),
        "reason": reason,
        "completed_count": completed_count,
        "results": results,
        "cost_summary": cost_tracker.get_summary() if cost_tracker else None
    }

    checkpoint_file = Path("state/checkpoint_latest.json")
    checkpoint_file.parent.mkdir(parents=True, exist_ok=True)

    with open(checkpoint_file, 'w') as f:
        json.dump(checkpoint, f, indent=2)

    # Also save timestamped backup
    backup_file = Path(f"state/checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(backup_file, 'w') as f:
        json.dump(checkpoint, f, indent=2)

def resume_from_checkpoint(checkpoint_file):
    """Resume processing from checkpoint."""
    with open(checkpoint_file) as f:
        checkpoint = json.load(f)

    logger.info("=" * 60)
    logger.info("RESUMING FROM CHECKPOINT")
    logger.info("=" * 60)
    logger.info(f"Checkpoint time: {checkpoint['timestamp']}")
    logger.info(f"Completed: {checkpoint['completed_count']} brands")
    logger.info(f"Success: {len(checkpoint['results']['success'])}")
    logger.info(f"Failed: {len(checkpoint['results']['failed'])}")

    return checkpoint
```

**Expected Impact:**
- üõ°Ô∏è **Reliability:** Never lose more than 50 brands of work
- üîÑ **Resume:** Fast resume from any interruption

---

## üìà Performance Comparison

### **Before Optimization (Current)**
```
500 Brands Processing:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Method: Sequential (one brand at a time)        ‚îÇ
‚îÇ Time: 5.5 hours (20,000 seconds)                ‚îÇ
‚îÇ Cost: $25.00                                     ‚îÇ
‚îÇ API Calls: ~4,000 total                          ‚îÇ
‚îÇ Success Rate: 95%                                ‚îÇ
‚îÇ Progress Visibility: ‚ùå None                     ‚îÇ
‚îÇ Cost Tracking: ‚ùå None                           ‚îÇ
‚îÇ Checkpoint: ‚ö†Ô∏è Only on errors                   ‚îÇ
‚îÇ Rate Limiting: ‚ùå None                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **After Optimization (Proposed)**
```
500 Brands Processing:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Method: Parallel (8 workers)                    ‚îÇ
‚îÇ Time: 45-90 minutes (2,700-5,400 seconds)       ‚îÇ
‚îÇ Cost: $25.00 (same)                              ‚îÇ
‚îÇ API Calls: ~4,000 total (same)                   ‚îÇ
‚îÇ Success Rate: 95%+ (same or better)              ‚îÇ
‚îÇ Progress Visibility: ‚úÖ Real-time with ETA       ‚îÇ
‚îÇ Cost Tracking: ‚úÖ Live budget monitoring         ‚îÇ
‚îÇ Checkpoint: ‚úÖ Every 50 brands + on error        ‚îÇ
‚îÇ Rate Limiting: ‚úÖ Adaptive throttling            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ö° 6-7√ó FASTER (5.5 hours ‚Üí 45-90 minutes)
```

---

## üéØ Implementation Phases

### **Phase 1: Critical Performance (Week 1)** üî¥

**Goal:** Reduce time from 5.5 hours to ~90 minutes

1. ‚úÖ **Parallel Processing**
   - Implement worker pool with 8 workers
   - Add chunk processing for memory management
   - Test with 50 brands first

2. ‚úÖ **One-Time Validation**
   - Move API validation to batch start
   - Add batch-level fail-fast

3. ‚úÖ **Progress Tracking**
   - Add tqdm progress bar
   - Show ETA and success/failed counts

**Expected Result:** 500 brands in ~90 minutes

---

### **Phase 2: Reliability & Visibility (Week 2)** üü†

**Goal:** Make batch processing robust and transparent

1. ‚úÖ **Rate Limiting**
   - Implement RateLimiter class
   - Add adaptive throttling
   - Configure limits for all APIs

2. ‚úÖ **Enhanced Checkpoints**
   - Auto-checkpoint every 50 brands
   - Add timestamped backups
   - Improve resume logic

3. ‚úÖ **Cost Tracking**
   - Implement CostTracker class
   - Add budget limits
   - Show real-time cost in progress bar

**Expected Result:** Reliable, transparent batch processing

---

### **Phase 3: Advanced Optimizations (Week 3)** üü°

**Goal:** Further optimize for maximum efficiency

1. ‚úÖ **Batch Sheets Updates**
   - Queue updates in memory
   - Flush every 20 brands
   - Reduce API calls by 95%

2. ‚úÖ **Smart Caching**
   - Cache industry taxonomies
   - Cache common LLM responses
   - Reduce redundant API calls

3. ‚úÖ **Performance Monitoring**
   - Track bottlenecks per agent
   - Log slow brands
   - Generate performance report

**Expected Result:** 500 brands in ~45 minutes (best case)

---

## üíª Usage Examples

### **Current (Sequential)**
```bash
# Process all brands from Google Sheets
python main.py --google-sheets

# Output:
# Processing 500 brands...
# [1/500] Brand1... ‚úì (42.3s)
# [2/500] Brand2... ‚úì (38.7s)
# ...
# [500/500] Brand500... ‚úì (41.2s)
# Total time: 5.5 hours
```

### **Proposed (Parallel with Budget)**
```bash
# Process with 8 workers and $50 budget limit
python main.py --google-sheets --workers 8 --budget 50.0

# Output with real-time progress:
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          BATCH MODE - PARALLEL PROCESSING                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

[INIT] Reading brands from Google Sheets...
[INIT] Found 500 brands (475 need processing, 25 unchanged)
[INIT] Configuration:
  - Workers: 8 parallel
  - Chunk size: 50 brands
  - Budget limit: $50.00
  - Auto-checkpoint: Every 50 brands

[VALIDATION] Validating API keys...
  ‚úÖ OpenRouter API key valid
  ‚úÖ Tavily API key valid
  ‚úÖ Google Sheets credentials valid

[START] Processing 475 brands...
Estimated time: 60-75 minutes

Processing: 24%|‚ñà‚ñà‚ñà‚ñà‚ñé              | 115/475 [18:32<56:28, 9.41s/brand]
‚úì Success: 110 | ‚úó Failed: 5 | üí∞ Cost: $5.75/$50.00 | ‚è±Ô∏è ETA: 56.5m

[CHECKPOINT] Auto-save at 50 brands... ‚úì
[CHECKPOINT] Auto-save at 100 brands... ‚úì

Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 475/475 [62:15<00:00, 7.86s/brand]
‚úì Success: 468 | ‚úó Failed: 7 | üí∞ Cost: $23.75/$50.00 | ‚è±Ô∏è Done!

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    BATCH COMPLETE                            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

[SUMMARY]
  Total Brands: 500
  Processed: 475 (25 skipped - unchanged)
  Success: 468 (98.5%)
  Failed: 7 (1.5%)

[TIME]
  Total: 62 minutes 15 seconds
  Average: 7.86 seconds per brand
  Speedup: 5.3√ó faster than sequential

[COST]
  Tavily: $14.25
  OpenRouter: $9.50
  Total: $23.75 (47.5% of budget)

[OUTPUTS]
  Reports: 468 brands √ó 4 formats = 1,872 files
  Directory: output/batch_2026-02-17_14-30/

[LOGS]
  Daily log: logs/batch_2026-02-17.log
  Structured log: logs/batch_2026-02-17.jsonl
  Final checkpoint: state/checkpoint_latest.json

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```

---

## üß™ Testing Plan

### **1. Small Batch Test (10 brands)**
```bash
# Test parallel processing with 10 brands
python main.py --google-sheets --workers 4 --limit 10

# Verify:
# - All 10 brands processed
# - No data corruption
# - Correct file outputs
# - Progress bar works
```

### **2. Medium Batch Test (50 brands)**
```bash
# Test with checkpoint and budget
python main.py --google-sheets --workers 8 --limit 50 --budget 5.0

# Verify:
# - Checkpoint saved at 50 brands
# - Cost tracking accurate
# - Resume works correctly
```

### **3. Full Batch Test (500 brands)**
```bash
# Production run
python main.py --google-sheets --workers 8 --budget 50.0

# Monitor:
# - Processing time
# - Success rate
# - Cost accuracy
# - Memory usage
# - API rate limits
```

### **4. Stress Test**
```bash
# Test with API failures
# - Simulate rate limiting
# - Simulate API disconnection
# - Test checkpoint recovery
# - Test budget exceeded scenario
```

---

## üìä Expected Results

### **Time Savings**

| Brands | Current (Sequential) | Optimized (8 Workers) | Speedup |
|--------|---------------------|----------------------|---------|
| 10     | 7 minutes           | 1 minute             | 7√ó      |
| 50     | 33 minutes          | 5 minutes            | 6.6√ó    |
| 100    | 67 minutes          | 10 minutes           | 6.7√ó    |
| 500    | 333 minutes (5.5h)  | 50 minutes           | 6.7√ó    |
| 1000   | 667 minutes (11h)   | 100 minutes (1.7h)   | 6.7√ó    |

### **Cost (Unchanged)**

| Brands | Tavily | OpenRouter | Total |
|--------|--------|------------|-------|
| 10     | $0.30  | $0.20      | $0.50 |
| 50     | $1.50  | $1.00      | $2.50 |
| 100    | $3.00  | $2.00      | $5.00 |
| 500    | $15.00 | $10.00     | $25.00|
| 1000   | $30.00 | $20.00     | $50.00|

---

## ‚ö†Ô∏è Risks & Mitigations

### **Risk 1: API Rate Limits**
- **Mitigation:** Adaptive rate limiting (RateLimiter class)
- **Fallback:** Reduce workers if rate limited

### **Risk 2: Memory Usage**
- **Mitigation:** Process in chunks (50 brands per chunk)
- **Fallback:** Reduce chunk size or workers

### **Risk 3: Google Sheets API Quota**
- **Mitigation:** Batch updates (20 brands per call)
- **Fallback:** Increase batch size to 50

### **Risk 4: Partial Failures**
- **Mitigation:** Enhanced checkpoints every 50 brands
- **Fallback:** Resume from last checkpoint

### **Risk 5: Budget Overrun**
- **Mitigation:** Real-time cost tracking with hard limits
- **Fallback:** Stop processing if budget exceeded

---

## üéØ Success Metrics

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| **Processing Time (500 brands)** | 5.5 hours | 45-90 min | üéØ 6-7√ó improvement |
| **Cost per Brand** | $0.05 | $0.05 | ‚úÖ Unchanged |
| **Success Rate** | 95% | 95%+ | ‚úÖ Same or better |
| **API Call Efficiency** | 100% | 95% | ‚úÖ Smart caching |
| **User Visibility** | None | Real-time | ‚úÖ Progress + ETA |
| **Cost Transparency** | None | Live tracking | ‚úÖ Budget monitoring |
| **Reliability** | Checkpoint on error | Auto-checkpoint | ‚úÖ Every 50 brands |

---

## üìù Summary

### **Key Improvements**
1. ‚ö° **6-7√ó Faster:** 5.5 hours ‚Üí 45-90 minutes
2. üí∞ **Same Cost:** $25 for 500 brands (no increase)
3. üìä **Real-time Visibility:** Progress bar with ETA
4. üíµ **Budget Control:** Live cost tracking with limits
5. üõ°Ô∏è **Enhanced Reliability:** Auto-checkpoint every 50 brands
6. üö¶ **Rate Limiting:** Prevents API throttling
7. üìà **Batch Updates:** 95% reduction in Google Sheets calls

### **Implementation Timeline**
- **Week 1:** Parallel processing (6√ó speedup)
- **Week 2:** Reliability features (checkpoints, rate limiting)
- **Week 3:** Advanced optimizations (caching, batch updates)

### **Final Result**
Process **500 brands in under 1 hour** with full visibility, cost control, and reliability.

---

**Ready to implement?** Let me know which phase to start with! üöÄ

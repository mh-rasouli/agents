# Brand Intelligence Agent ğŸ”

A Multi-Agent System for comprehensive Iranian brand analysis, designed for advertising agencies to make data-driven campaign decisions.

## Overview

This system leverages **LangGraph** and **OpenRouter API (Gemini 3 Pro)** to orchestrate 6 specialized AI agents that collect, analyze, and synthesize brand intelligence from multiple Iranian data sources.

### Key Features

- ğŸ“Š **Multi-Source Data Collection**: Aggregates data from 7+ Iranian sources (rasmio, codal, tsetmc, linka, trademark registry, websites)
- ğŸ” **AI-Powered Search**: Optional Tavily integration for enhanced search results
- ğŸ”— **Relationship Mapping**: Identifies parent companies, subsidiaries, sister brands, and shareholders
- ğŸ¢ **Industry Categorization**: Classifies industries, products, audience segments, and price tiers
- ğŸ’¡ **Strategic Insights**: Generates actionable advertising recommendations
- ğŸ“„ **Multi-Format Output**: Produces JSON, CSV, TXT (embedding-ready), and Markdown reports

## Architecture

### 5 Specialized Agents

1. **DataCollectionAgent** - Orchestrates web scraping from all sources
2. **RelationshipMappingAgent** - Analyzes corporate structure
3. **CategorizationAgent** - Classifies industries and market positioning
4. **StrategicInsightsAgent** - Generates advertising recommendations
5. **OutputFormatterAgent** - Produces multi-format outputs

### Workflow Pipeline

```
START â†’ Data Collection â†’ Relationship Mapping â†’ Categorization â†’ Strategic Insights â†’ Output Formatting â†’ END
```

## Installation

### Prerequisites

- Python 3.11 or higher
- OpenRouter API key ([Get one here](https://openrouter.ai/keys))

### Setup

1. **Clone or download this repository**

2. **Create virtual environment**

```bash
python -m venv venv

# On Windows
venv\Scripts\activate

# On Linux/Mac
source venv/bin/activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Configure environment variables**

```bash
# Copy example env file
cp .env.example .env

# Edit .env and add your OpenRouter API key
# OPENROUTER_API_KEY=your_api_key_here
# MODEL_NAME=google/gemini-pro-1.5
```

5. **(Optional) Install Playwright browsers** (for JavaScript-heavy sites)

```bash
playwright install
```

6. **(Optional) Enable Tavily AI Search** (for enhanced search results)

```bash
# Get API key from https://tavily.com (1,000 free searches/month)
# Add to .env:
TAVILY_ENABLED=true
TAVILY_API_KEY=tvly-your-api-key-here
```

See [TAVILY_INTEGRATION.md](TAVILY_INTEGRATION.md) for details.

## Usage

### Basic Usage

```bash
# Analyze a brand by name
python main.py --brand "Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§"

# Analyze with website URL
python main.py --brand "Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§" --website "https://www.digikala.com"
```

### Advanced Options

```bash
# Specify custom output directory
python main.py --brand "Ø§Ø³Ù†Ù¾" --output-dir "./reports"

# Select specific output formats
python main.py --brand "Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§" --formats json,md

# Verbose logging
python main.py --brand "Ø§Ø³Ù†Ù¾" --verbose
```

### Command-Line Arguments

| Argument | Short | Description | Default |
|----------|-------|-------------|---------|
| `--brand` | `-b` | Brand name to analyze (required) | - |
| `--website` | `-w` | Brand website URL (optional) | - |
| `--output-dir` | `-o` | Output directory | `./output` |
| `--formats` | `-f` | Output formats (comma-separated) | `json,csv,txt,md` |
| `--verbose` | `-v` | Enable verbose logging | `False` |

## Output Formats

### 1. JSON (`.json`)
Structured nested data with all collected information, relationships, categorization, and insights.

```json
{
  "brand_name": "Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§",
  "data": {...},
  "relationships": {...},
  "categorization": {...},
  "insights": {...}
}
```

### 2. CSV (`.csv`)
Flattened tabular data suitable for spreadsheet analysis.

### 3. TXT (`.txt`)
Embedding-ready key-value format for vector databases:

```
brand_name: Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§
industry: E-commerce
business_model: B2C
...
```

### 4. Markdown (`.md`)
Executive summary report with formatted sections.

## Data Sources

| Source | Type | Data Collected |
|--------|------|----------------|
| **rasmio.com** | Company Registry | Legal name, registration number, capital |
| **codal.ir** | Financial Statements | Revenue, profit, assets, fiscal data |
| **tsetmc.com** | Stock Market | Ticker, market cap, stock price |
| **linka.ir** | Social Media | Follower counts, engagement metrics |
| **Trademark Registry** | Intellectual Property | Registered brands, parent company |
| **Official Website** | Web Scraping | About, products, contact info |

## Project Structure

```
brand-intelligence-agent/
â”œâ”€â”€ agents/              # 5 specialized agents
â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”œâ”€â”€ data_collection_agent.py
â”‚   â”œâ”€â”€ relationship_agent.py
â”‚   â”œâ”€â”€ categorization_agent.py
â”‚   â”œâ”€â”€ insights_agent.py
â”‚   â””â”€â”€ formatter_agent.py
â”œâ”€â”€ scrapers/            # 6 web scrapers
â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”œâ”€â”€ rasmio_scraper.py
â”‚   â”œâ”€â”€ codal_scraper.py
â”‚   â”œâ”€â”€ tsetmc_scraper.py
â”‚   â”œâ”€â”€ linka_scraper.py
â”‚   â”œâ”€â”€ trademark_scraper.py
â”‚   â””â”€â”€ web_search.py
â”œâ”€â”€ models/              # State definitions
â”‚   â””â”€â”€ state.py
â”œâ”€â”€ utils/               # Utilities
â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ config/              # Configuration
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ prompts.py
â”œâ”€â”€ output/              # Generated reports
â”œâ”€â”€ data/                # Cached scraped data
â”œâ”€â”€ graph.py             # LangGraph workflow
â”œâ”€â”€ main.py              # CLI entry point
â””â”€â”€ requirements.txt
```

## Configuration

Edit `.env` to customize settings:

```bash
# API Configuration
ANTHROPIC_API_KEY=your_key_here
MODEL_NAME=claude-sonnet-4-5-20250929
MAX_TOKENS=4096

# Scraper Settings
RATE_LIMIT_DELAY=1.5     # Seconds between requests
SCRAPER_TIMEOUT=30       # Request timeout
CACHE_TTL_HOURS=24       # Cache validity period

# Logging
LOG_LEVEL=INFO
```

## Features

### Intelligent Caching
- Scraped data is cached for 24 hours (configurable)
- Reduces redundant requests
- Speeds up repeated analyses

### Rate Limiting
- Respects robots.txt
- Configurable delays between requests
- Prevents server overload

### Error Handling
- Graceful degradation when scrapers fail
- Comprehensive error logging
- Continues processing with available data

### Multi-Format Export
- JSON for structured data
- CSV for spreadsheet analysis
- TXT for vector embeddings
- Markdown for executive reports

## Example Output

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Brand Intelligence Agent - Multi-Agent System        â•‘
â•‘              Iranian Brand Analysis for Advertising          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

============================================================
ANALYSIS COMPLETE
============================================================

Brand: Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§

ğŸ“ Generated Reports:
  ğŸ“Š JSON: output/digikala_20260215_143022.json
  ğŸ“ˆ CSV: output/digikala_20260215_143022.csv
  ğŸ“ TXT: output/digikala_20260215_143022.txt
  ğŸ“„ MARKDOWN: output/digikala_20260215_143022.md

ğŸ¤ Cross-Promotion Opportunities:
  â€¢ Ø¯ÛŒØ¬ÛŒâ€ŒÙ¾ÛŒ
  â€¢ Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§ Ø¬Øª
  â€¢ ÙÛŒØ¯ÛŒØ¨Ùˆ

ğŸ“¢ Top Recommended Channels:
  ğŸ”´ Digital - Instagram, Telegram
  ğŸŸ¡ TV - National Networks
  ğŸŸ¢ Outdoor - Tehran Metro

ğŸ¢ Industry: ØªØ¬Ø§Ø±Øª Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©
ğŸ’¼ Business Model: B2C
ğŸ’° Price Tier: mid-market

â±ï¸  Processing Time: 45.32 seconds
============================================================
```

## API Usage

You can also use the workflow programmatically:

```python
from graph import run_workflow

# Run analysis
result = run_workflow(
    brand_name="Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§",
    brand_website="https://www.digikala.com"
)

# Access results
insights = result["insights"]
relationships = result["relationships"]
```

## Limitations & Known Issues

1. **Placeholder Scrapers**: Current scraper implementations are placeholders. Actual scraping logic needs to be implemented based on each website's structure.

2. **Persian Number Parsing**: Some scrapers may need additional logic to parse Persian/Arabic numerals.

3. **CAPTCHA**: System cannot bypass CAPTCHA challenges (fails gracefully).

4. **Rate Limits**: Some websites may block requests if rate limits are exceeded.

## Future Enhancements

- [ ] Web dashboard (FastAPI + React)
- [ ] Database storage (PostgreSQL)
- [ ] Vector database integration (ChromaDB)
- [ ] Real-time monitoring
- [ ] Multi-brand batch processing
- [ ] Historical trend analysis
- [ ] API endpoint exposure

## Contributing

Contributions are welcome! Areas for improvement:

1. Implement actual scraping logic for each source
2. Add Persian number parsing utilities
3. Improve error handling
4. Add unit tests
5. Enhance LLM prompts

## License

MIT License - feel free to use and modify for your needs.

## Support

For issues, questions, or suggestions:
- Open an issue on GitHub
- Contact: [your-email@example.com]

## Acknowledgments

- Built with [LangGraph](https://github.com/langchain-ai/langgraph)
- Powered by [Claude API](https://www.anthropic.com/claude)
- Designed for Iranian advertising agencies

---

**Made with â¤ï¸ for the Iranian advertising industry**
